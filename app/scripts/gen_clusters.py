# Applying the requested changes to use centralized route functions from settings in the save_clustering_summary function.
import os, sys
import pandas as pd
import numpy as np
import sqlite3
import json
import re
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import RobustScaler
from kneed import KneeLocator
from sklearn.metrics import silhouette_score

# Definim entorn on s'executarÃ  aquest script (com si fos el root)
base_dir = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(base_dir))

# Importem funcions necessÃ ries
from app.scripts.utils import prompt_AI, get_clean_table_name, get_user_db_path, get_user_folder_path

# Constants
DATABASE_DIR = 'app/users/'
DEFAULT_DB_NAME = 'user_database'

def create_user_table_from_file(file_path, username):
    """
    Carrega un fitxer CSV/Excel a una base de dades SQLite de l'usuari
    i retorna el nom de la taula creada
    """
    db_path = get_user_db_path(username)
    conn = sqlite3.connect(db_path)
    table_name = get_clean_table_name(file_path)

    # Determinar format del fitxer
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()

    if ext in ['.xlsx', '.xls']:
        df = pd.read_excel(file_path)
    elif ext == '.csv':
        # Provar diferents separadors
        for sep in [',', ';', '\t', '|']:
            try:
                df = pd.read_csv(file_path, sep=sep)
                if df.shape[1] > 1:
                    break
            except Exception:
                continue
        else:
            raise ValueError("No s'ha pogut detectar el separador del CSV.")
    else:
        raise ValueError("Format de fitxer no suportat.")

    # Neteja bÃ sica del DataFrame
    df.columns = df.columns.str.strip()
    df = df.loc[:, df.columns.notna()]
    df = df.loc[:, df.columns != '']

    # Netejar strings
    for col in df.select_dtypes(include=['object']):
        df[col] = df[col].astype(str).str.strip()

    # Guardar a SQLite
    df.to_sql(table_name, conn, if_exists='replace', index=False)
    conn.close()

    print(f"âœ… Taula '{table_name}' creada correctament")
    return table_name

def get_table_info(username, table_name):
    """
    ObtÃ© informaciÃ³ de la taula per generar prompts d'IA
    """
    db_path = get_user_db_path(username)
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Obtenir informaciÃ³ de columnes
    cursor.execute(f"PRAGMA table_info({table_name})")
    columns_info = cursor.fetchall()

    # Obtenir una mostra de dades
    cursor.execute(f"SELECT * FROM {table_name} LIMIT 5")
    sample_data = cursor.fetchall()

    # Crear text descriptiu
    headers = [col[1] for col in columns_info]
    headers_text = ", ".join(headers)

    sample_text = ""
    for row in sample_data:
        sample_text += ", ".join([str(val) for val in row]) + "\n"

    conn.close()

    return {
        'headers': headers,
        'headers_text': headers_text,
        'sample_data': sample_text,
        'columns_info': columns_info
    }

def generate_clustering_table_sql(username, original_table_name):
    """
    Genera SQL amb IA per crear una taula de clustering modificada
    """
    table_info = get_table_info(username, original_table_name)

    clustering_table_name = f"{original_table_name}_clustering"

    prompt = f"""
Ets un expert en SQL i anÃ¡lisis de dades. 

Tens una taula anomenada '{original_table_name}' amb les segÃ¼ents columnes:
{table_info['headers_text']}

Mostra de dades:
{table_info['sample_data']}

Genera una sentÃ¨ncia SQL CREATE TABLE per crear una nova taula anomenada '{clustering_table_name}' 
que contingui NOMÃ‰S les columnes mÃ©s rellevants per fer clustering (anÃ¡lisis de segments).

Regles:
1. Selecciona nomÃ©s columnes numÃ¨riques o categÃ²riques Ãºtils per clustering
2. Evita IDs, noms o columnes identificatives
3. Inclou columnes que puguin revelar patrons de comportament
4. La sentÃ¨ncia ha de ser: CREATE TABLE {clustering_table_name} AS SELECT ... FROM {original_table_name};
5. No afegeixis comentaris, nomÃ©s el SQL

Retorna nomÃ©s la sentÃ¨ncia SQL:
"""

    sql_response = prompt_AI(prompt)

    # Netejar la resposta
    sql_clean = re.search(r'CREATE TABLE.*?;', sql_response, re.DOTALL | re.IGNORECASE)
    if sql_clean:
        return sql_clean.group(0).strip()
    else:
        raise ValueError("No s'ha pogut generar SQL vÃ lid")

def generate_feature_engineering_sql(username, clustering_table_name):
    """
    Genera SQL amb IA per modificar la taula de clustering amb noves columnes calculades
    """
    table_info = get_table_info(username, clustering_table_name)

    prompt = f"""
Ets un expert en feature engineering per clustering.

Tens una taula de clustering anomenada '{clustering_table_name}' amb columnes:
{table_info['headers_text']}

Mostra de dades:
{table_info['sample_data']}

Genera sentÃ¨ncies SQL ALTER TABLE per afegir MÃ€XIM 2 noves columnes calculades Ãºtils per clustering.
Exemples: rÃ tios entre variables, diferÃ¨ncies, transformacions matemÃ tiques.

Regles:
1. MÃ xim 2 columnes noves
2. NomÃ©s columnes realment Ãºtils per segmentaciÃ³
3. Utilitza funcions SQL estÃ ndard
4. Format: ALTER TABLE {clustering_table_name} ADD COLUMN nom_columna AS (cÃ lcul);
5. Si no Ã©s possible calcular columnes Ãºtils, retorna nomÃ©s: -- NO NEW COLUMNS --

Retorna nomÃ©s les sentÃ¨ncies SQL (una per lÃ­nia):
"""

    sql_response = prompt_AI(prompt)

    if "-- NO NEW COLUMNS --" in sql_response:
        print("ğŸ” No es poden generar noves columnes calculades")
        return None

    # Extreure sentÃ¨ncies ALTER TABLE
    alter_statements = re.findall(r'ALTER TABLE.*?;', sql_response, re.DOTALL | re.IGNORECASE)
    if alter_statements:
        return '\n'.join(alter_statements)
    else:
        print("ğŸ” No s'han trobat sentÃ¨ncies ALTER TABLE vÃ lides")
        return None

def execute_sql_statements(username, sql_statements):
    """
    Executa sentÃ¨ncies SQL a la base de dades de l'usuari
    """
    if not sql_statements:
        return True

    db_path = get_user_db_path(username)
    conn = sqlite3.connect(db_path)

    try:
        # Dividir en sentÃ¨ncies individuals si hi ha mÃºltiples
        statements = [stmt.strip() for stmt in sql_statements.split(';') if stmt.strip()]

        for statement in statements:
            print(f"ğŸ”„ Executant: {statement[:100]}...")
            conn.execute(statement + ';')

        conn.commit()
        conn.close()
        print("âœ… SQL executat correctament")
        return True

    except Exception as e:
        conn.close()
        print(f"âŒ Error executant SQL: {e}")
        return False

def prepare_clustering_data(username, clustering_table_name):
    """
    Prepara les dades per al clustering des de la taula SQL
    """
    db_path = get_user_db_path(username)
    conn = sqlite3.connect(db_path)

    try:
        df = pd.read_sql_query(f"SELECT * FROM {clustering_table_name}", conn)
        conn.close()
        print(f"âœ… Dades carregades: {df.shape[0]} files, {df.shape[1]} columnes")
        return df
    except Exception as e:
        conn.close()
        print(f"âŒ Error carregant dades: {e}")
        return None

def handle_high_cardinality_categoricals(df, max_card=20, top_n=10):
    """Agrupa categories poc freqÃ¼ents en 'Other' i exclou variables amb cardinalitat excessiva."""
    df_result = df.copy()
    categoricals = [col for col in df_result.select_dtypes(include=['object', 'category']).columns]
    cols_to_use = []

    for col in categoricals:
        num_unique = df_result[col].nunique(dropna=False)
        if num_unique > max_card:
            continue
        top_cats = df_result[col].value_counts().nlargest(top_n).index
        df_result[col] = df_result[col].apply(lambda x: x if x in top_cats else 'Other')
        cols_to_use.append(col)

    return df_result, cols_to_use

def detect_optimal_clusters(X, min_k=2, max_k=6):
    """Detecta el nombre Ã²ptim de clusters"""
    if len(X) < min_k:
        return min_k

    # MÃ¨tode del colze
    costs = []
    for k in range(min_k, min(max_k + 1, len(X))):
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
        kmeans.fit(X)
        costs.append(kmeans.inertia_)

    if len(costs) > 1:
        kl = KneeLocator(range(min_k, min_k + len(costs)), costs, curve="convex", direction="decreasing")
        if kl.elbow is not None:
            return max(min_k, min(kl.elbow, max_k))

    # Si no hi ha colze clar, usar silhouette
    silhouette_scores = []
    for k in range(min_k, min(max_k + 1, len(X))):
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = kmeans.fit_predict(X)
        if len(set(labels)) > 1:
            score = silhouette_score(X, labels)
        else:
            score = -1
        silhouette_scores.append(score)

    return np.argmax(silhouette_scores) + min_k

def perform_clustering(df, n_clusters=0):
    """
    Realitza el clustering sobre el DataFrame preparat
    """
    # Processar variables categÃ²riques
    df_proc, categoricals = handle_high_cardinality_categoricals(df)
    numerics = df_proc.select_dtypes(include=['number']).columns.tolist()

    # Crear variables dummy per categÃ²riques
    df_cat = pd.get_dummies(df_proc[categoricals], drop_first=False) if categoricals else pd.DataFrame(index=df_proc.index)

    # Escalar variables numÃ¨riques
    scaler = RobustScaler()
    X_num = scaler.fit_transform(df_proc[numerics]) if numerics else np.zeros((len(df_proc), 0))

    # Combinar tots els features
    X = np.concatenate([X_num, df_cat.values], axis=1) if df_cat.shape[1] > 0 else X_num

    # Determinar nombre de clusters
    if n_clusters == 0:
        n_clusters = detect_optimal_clusters(X)
        print(f"ğŸ¯ Nombre Ã²ptim de clusters detectat: {n_clusters}")

    # Aplicar KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df_result = df.copy()
    df_result['cluster'] = kmeans.fit_predict(X)

    # Mostrar distribuciÃ³ de clusters
    cluster_counts = df_result['cluster'].value_counts(normalize=True).sort_index()
    cluster_percentages = (cluster_counts * 100).round(2)

    print("\nğŸ“Š DistribuciÃ³ de clusters:")
    for cl, perc in cluster_percentages.items():
        print(f"  Cluster {cl}: {perc}%")

    return df_result, n_clusters

def generate_cluster_summary(df, n_clusters):
    """
    Genera un resum dels clusters
    """
    numerics = df.select_dtypes(include=['number']).columns.tolist()
    numerics = [col for col in numerics if col != 'cluster']  # Excloure la columna cluster
    categoricals = [col for col in df.select_dtypes(include=['object', 'category']).columns]

    cluster_summary = {}

    # EstadÃ­stiques globals
    global_num_mean = {col: float(df[col].mean()) for col in numerics}
    global_cat_dist = {col: df[col].value_counts(normalize=True).to_dict() for col in categoricals}

    for i in range(n_clusters):
        group = df[df['cluster'] == i]
        if len(group) == 0:
            continue

        cluster_info = {
            'cluster_id': int(i),
            'size': int(len(group)),
            'percentage': float(round(len(group) / len(df) * 100, 1))
        }

        # EstadÃ­stiques numÃ¨riques
        if numerics:
            num_stats = {}
            for col in numerics:
                cluster_mean = float(group[col].mean())
                global_mean = global_num_mean[col]
                if abs(cluster_mean - global_mean) >= 0.01:
                    num_stats[col] = {
                        'cluster_mean': round(cluster_mean, 2),
                        'global_mean': round(global_mean, 2),
                        'difference': round(cluster_mean - global_mean, 2)
                    }
            if num_stats:
                cluster_info['numeric_features'] = num_stats

        # EstadÃ­stiques categÃ²riques
        if categoricals:
            cat_stats = {}
            for col in categoricals:
                cluster_dist = group[col].value_counts(normalize=True)
                global_dist = global_cat_dist[col]

                significant_categories = {}
                for cat in cluster_dist.index:
                    cluster_pct = cluster_dist[cat] * 100
                    global_pct = global_dist.get(cat, 0) * 100
                    if abs(cluster_pct - global_pct) >= 5:  # DiferÃ¨ncia significativa
                        significant_categories[cat] = {
                            'cluster_pct': round(cluster_pct, 1),
                            'global_pct': round(global_pct, 1),
                            'difference': round(cluster_pct - global_pct, 1)
                        }

                if significant_categories:
                    cat_stats[col] = significant_categories

            if cat_stats:
                cluster_info['categorical_features'] = cat_stats

        cluster_summary[f"cluster_{i}"] = cluster_info

    return cluster_summary

def save_clustering_summary(username, clustering_summary, table_name):
    """
    Guarda el resum de clustering com a JSON a la carpeta de l'usuari
    """
    from django.conf import settings
    user_clusters_folder = settings.get_user_clusters_dir(username)
    summary_path = os.path.join(str(user_clusters_folder), f'{table_name}_clustering_summary.json')

def save_clustering_results(username, table_name, df_clustered, cluster_summary):
    """
    Guarda els resultats del clustering
    """
    user_folder = get_user_folder_path(username)
    clusters_folder = get_user_folder_path(username, 'clusters')

    # Guardar DataFrame amb clusters a SQLite
    db_path = get_user_db_path(username)
    conn = sqlite3.connect(db_path)
    df_clustered.to_sql(f"{table_name}_clustering_results", conn, if_exists='replace', index=False)
    conn.close()

    # Guardar resum en JSON a la carpeta clusters
    json_filename = f"{table_name}_clustering_summary.json"
    json_path = os.path.join(clusters_folder, json_filename)

    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(cluster_summary, f, indent=2, ensure_ascii=False)

    print(f"âœ… Resultats guardats:")
    print(f"   - Taula SQL: {table_name}_clustering_results")
    print(f"   - Resum JSON: {json_filename}")

    return json_path

def full_clustering_pipeline(file_path, username, n_clusters=0):
    """
    Pipeline complet de clustering amb generaciÃ³ SQL amb IA
    """
    print("ğŸš€ Iniciant pipeline de clustering...")

    try:
        # 1. Crear taula original
        print("\nğŸ“ Pas 1: Carregant fitxer a SQLite...")
        original_table_name = create_user_table_from_file(file_path, username)

        # 2. Generar taula de clustering amb IA
        print("\nğŸ¤– Pas 2: Generant taula de clustering amb IA...")
        clustering_sql = generate_clustering_table_sql(username, original_table_name)
        print(f"SQL generat: {clustering_sql}")

        if not execute_sql_statements(username, clustering_sql):
            raise Exception("Error executant SQL de taula de clustering")

        clustering_table_name = f"{original_table_name}_clustering"

        # 3. Feature engineering amb IA
        print("\nğŸ”§ Pas 3: Feature engineering amb IA...")
        feature_sql = generate_feature_engineering_sql(username, clustering_table_name)
        if feature_sql:
            print(f"Feature engineering SQL: {feature_sql}")
            execute_sql_statements(username, feature_sql)

        # 4. Carregar dades per clustering
        print("\nğŸ“Š Pas 4: Preparant dades per clustering...")
        df = prepare_clustering_data(username, clustering_table_name)
        if df is None:
            raise Exception("Error carregant dades de clustering")

        # 5. Realizar clustering
        print("\nğŸ¯ Pas 5: Executant clustering...")
        df_clustered, n_clusters_used = perform_clustering(df, n_clusters)

        # 6. Generar resum
        print("\nğŸ“‹ Pas 6: Generant resum de clusters...")
        cluster_summary = generate_cluster_summary(df_clustered, n_clusters_used)

        # 7. Guardar resultats
        print("\nğŸ’¾ Pas 7: Guardant resultats...")
        json_path = save_clustering_results(username, original_table_name, df_clustered, cluster_summary)

        print(f"\nğŸ‰ Pipeline completat exitosament!")
        print(f"ğŸ“„ Resum guardat a: {json_path}")

        return cluster_summary, json_path

    except Exception as e:
        print(f"\nâŒ Error en el pipeline: {str(e)}")
        raise e

# FunciÃ³ per mantenir compatibilitat amb el codi existent
def full_clustering_flow(file_path, username, n_clusters=0):
    """Wrapper per compatibilitat amb el codi existent"""
    return full_clustering_pipeline(file_path, username, n_clusters)